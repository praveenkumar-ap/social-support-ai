services:
  api:
    build:
      context: /Users/praveen/social-support-ai
      dockerfile: Dockerfile
    command:
      - uvicorn
      - src.api.main:app
      - --host
      - 0.0.0.0
      - --port
      - "8000"
    depends_on:
      chromadb:
        condition: service_started
        required: true
      db:
        condition: service_started
        required: true
      llm:
        condition: service_started
        required: true
    environment:
      API_URL: http://localhost:8001
      BANK_STATEMENTS_DIR: data/raw/bank_statements
      BANK_STATEMENTS_OUTPUT: data/processed/bank_statements.csv
      CHROMA_URL: http://chromadb:8000
      CREDIT_REPORTS_DIR: data/raw/credit_reports
      CREDIT_REPORTS_OUTPUT: data/processed/credit_reports.csv
      ELIGIBILITY_MODEL_PATH: src/models/eligibility_model.pkl
      ELIGIBILITY_TRAINING_CSV: data/processed/eligibility_training.csv
      INPUT_IMAGES_DIR: data/raw/images
      LLM_HOST_URL: http://llm:11434
      OLLAMA_MODEL: llama2
      OUTPUT_IMAGES_DIR: data/processed/images
      POSTGRES_URL: postgresql://postgres:postgres@db:5432/social_support
      RECOMMEND_DOC_THRESHOLD: "2"
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8000
        published: "8001"
        protocol: tcp
    volumes:
      - type: bind
        source: /Users/praveen/social-support-ai
        target: /app
        bind:
          create_host_path: true
  chromadb:
    image: ghcr.io/chroma-core/chroma:latest
    networks:
      default: null
    volumes:
      - type: volume
        source: chroma-data
        target: /data
        volume: {}
  db:
    environment:
      POSTGRES_DB: social_support
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
    image: postgres:13
    networks:
      default: null
    volumes:
      - type: volume
        source: db-data
        target: /var/lib/postgresql/data
        volume: {}
  llm:
    command:
      - serve
    environment:
      OLLAMA_HOST: 0.0.0.0
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:11434/v1/models
      timeout: 1m0s
      interval: 30s
      retries: 3
    image: ollama/ollama:latest
    networks:
      default: null
    ports:
      - mode: ingress
        target: 11434
        published: "11434"
        protocol: tcp
    restart: unless-stopped
    volumes:
      - type: volume
        source: llm-data
        target: /root/.ollama
        volume: {}
  ui:
    command:
      - bash
      - -c
      - |2
          pip install --no-cache-dir -r requirements.txt &&
          streamlit run src/ui/app.py --server.port 8501 --server.enableCORS false
    depends_on:
      api:
        condition: service_started
        required: true
    environment:
      API_URL: http://api:8000
      PYTHONPATH: /app
    image: python:3.9-slim
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8501
        published: "8501"
        protocol: tcp
    volumes:
      - type: bind
        source: /Users/praveen/social-support-ai
        target: /app
        bind:
          create_host_path: true
    working_dir: /app
networks:
